{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "H100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuY_xHHnOYPa",
        "outputId": "b7b5c0d9-a362-4e5f-d6e6-8db34626997a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install git git-lfs curl -yqq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://huggingface.co/datasets/KernelCo/robot_control/ datasets/robot_control"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dox12qdtPBJA",
        "outputId": "bea46402-b481-49f4-9fcf-620d10359913"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'datasets/robot_control' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S7_RgC7QHSt",
        "outputId": "adfe4cbc-22cb-4a08-971d-effb636720be"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading uv 0.10.2 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
        "\n",
        "!uv pip install sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH8IcTMHPBLr",
        "outputId": "c970e65d-6920-434b-859f-4669ef0ada21"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 65ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 63ms\u001b[0m\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy.linalg import lstsq\n",
        "from glob import glob\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def P(msg=\"\"):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# CONFIG\n",
        "# =============================================\n",
        "\n",
        "# NIRS config\n",
        "NIRS_BASELINE_SAMPLES = 14      # ~3s at 4.76 Hz\n",
        "N_MODULES = 40\n",
        "SDS_LIST = [1, 2]               # medium + long distance channels\n",
        "WAVELENGTH = 1                  # IR (index 1)\n",
        "MOMENT = 1                      # mean time of flight (index 1)\n",
        "N_CHANNELS = N_MODULES * len(SDS_LIST)  # 80\n",
        "\n",
        "# Training config\n",
        "BATCH_SIZE = 256 # depends on machine arch\n",
        "EPOCHS = 3\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "PATIENCE = 20                   # early stopping\n",
        "N_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "# Model config\n",
        "D_MODEL = 64                    # transformer hidden dimension\n",
        "N_HEADS = 1                     # attention heads\n",
        "N_LAYERS = 5                    # transformer layers\n",
        "FF_DIM = 16                    # feedforward dimension\n",
        "DROPOUT = 0.3                   # regularization (aggressive for small dataset)\n",
        "\n",
        "# Augmentation\n",
        "AUG_NOISE_STD = 0.02            # Gaussian noise\n",
        "AUG_TIME_SHIFT = 3              # max time shift (samples)\n",
        "AUG_CHANNEL_DROP = 0.1          # probability of zeroing a channel\n",
        "\n",
        "\n",
        "def find_data_files(data_dir):\n",
        "    for pattern in [\n",
        "        os.path.join(data_dir, 'snapshots', '*', 'data', '*.npz'),\n",
        "        os.path.join(data_dir, '*.npz'),\n",
        "        os.path.join(data_dir, '**', '*.npz'),\n",
        "    ]:\n",
        "        files = sorted(glob(pattern, recursive=True))\n",
        "        if files:\n",
        "            return files\n",
        "    return []\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# NIRS PREPROCESSING\n",
        "# =============================================\n",
        "\n",
        "def preprocess_nirs(nirs_data):\n",
        "    \"\"\"\n",
        "    Short channel regression + baseline correction.\n",
        "    Input:  (72, 40, 3, 2, 3)\n",
        "    Output: (72, 80) — 40 modules × 2 SDS, using IR wavelength + mean TOF\n",
        "    \"\"\"\n",
        "    cleaned = nirs_data.copy().astype(np.float64)\n",
        "\n",
        "    # Short channel regression\n",
        "    for mod in range(N_MODULES):\n",
        "        for wl in range(2):\n",
        "            for mom in range(3):\n",
        "                short = cleaned[:, mod, 0, wl, mom]\n",
        "                if np.any(~np.isfinite(short)) or np.nanstd(short) < 1e-12:\n",
        "                    continue\n",
        "                short_2d = short.reshape(-1, 1)\n",
        "                for sds in SDS_LIST:\n",
        "                    brain = cleaned[:, mod, sds, wl, mom]\n",
        "                    if np.any(~np.isfinite(brain)) or np.nanstd(brain) < 1e-12:\n",
        "                        continue\n",
        "                    try:\n",
        "                        fit, _, _, _ = lstsq(short_2d, brain, rcond=None)\n",
        "                        result = brain - (short_2d @ fit).flatten()\n",
        "                        if np.all(np.isfinite(result)):\n",
        "                            cleaned[:, mod, sds, wl, mom] = result\n",
        "                    except np.linalg.LinAlgError:\n",
        "                        pass\n",
        "\n",
        "    # Baseline correction\n",
        "    baseline = np.nanmean(cleaned[:NIRS_BASELINE_SAMPLES], axis=0)\n",
        "    cleaned = cleaned - baseline\n",
        "\n",
        "    # Extract channels: 40 modules × 2 SDS (medium, long), IR, mean TOF\n",
        "    # Result shape: (72, 80)\n",
        "    channels = []\n",
        "    for mod in range(N_MODULES):\n",
        "        for sds in SDS_LIST:\n",
        "            ch = cleaned[:, mod, sds, WAVELENGTH, MOMENT]\n",
        "            ch = np.nan_to_num(ch, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "            channels.append(ch)\n",
        "\n",
        "    return np.stack(channels, axis=1)  # (72, 80)"
      ],
      "metadata": {
        "id": "EouRpOkvPBOA"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# DATASET\n",
        "# =============================================\n",
        "\n",
        "class NIRSDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for NIRS time-series data.\"\"\"\n",
        "\n",
        "    def __init__(self, data, labels, subjects, augment=False):\n",
        "        \"\"\"\n",
        "        data:     np.array (N, 72, 80) — preprocessed NIRS\n",
        "        labels:   np.array (N,) — integer labels\n",
        "        subjects: np.array (N,) — subject IDs (for subject-wise CV)\n",
        "        augment:  bool — whether to apply data augmentation\n",
        "        \"\"\"\n",
        "        self.data = torch.FloatTensor(data)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.subjects = subjects\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx].clone()  # (72, 80)\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            # 1. Gaussian noise\n",
        "            x = x + torch.randn_like(x) * AUG_NOISE_STD\n",
        "\n",
        "            # 2. Random time shift (circular)\n",
        "            shift = torch.randint(-AUG_TIME_SHIFT, AUG_TIME_SHIFT + 1, (1,)).item()\n",
        "            if shift != 0:\n",
        "                x = torch.roll(x, shifts=shift, dims=0)\n",
        "\n",
        "            # 3. Random channel dropout\n",
        "            mask = torch.rand(x.shape[1]) > AUG_CHANNEL_DROP\n",
        "            x = x * mask.unsqueeze(0)\n",
        "\n",
        "        return x, y\n"
      ],
      "metadata": {
        "id": "3Kh_wElCVJ8R"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# MODEL: NIRS TRANSFORMER\n",
        "# =============================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Learnable positional encoding for 72 timesteps.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        return x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class NIRSTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Small transformer for NIRS time-series classification.\n",
        "\n",
        "    Architecture:\n",
        "      Input (72, 80) → Conv1d patch embed → (72, d_model)\n",
        "                     → Positional encoding\n",
        "                     → Transformer Encoder (N layers)\n",
        "                     → Global Average Pooling → (d_model,)\n",
        "                     → Classification head → (5,)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels=80, n_classes=5, seq_len=72,\n",
        "                 d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
        "                 ff_dim=FF_DIM, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding: Conv1d to project 80 input channels → d_model\n",
        "        # Kernel size 5 captures ~1 second of NIRS data at 4.76 Hz\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv1d(n_channels, d_model, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=seq_len)\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True,  # Pre-norm (more stable for small datasets)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer, num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len=72, n_channels=80)\n",
        "        \"\"\"\n",
        "        # Conv1d expects (batch, channels, seq_len)\n",
        "        x = x.transpose(1, 2)          # (batch, 80, 72)\n",
        "        x = self.patch_embed(x)         # (batch, d_model, 72)\n",
        "        x = x.transpose(1, 2)          # (batch, 72, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.input_dropout(x)\n",
        "\n",
        "        # Transformer\n",
        "        x = self.transformer(x)        # (batch, 72, d_model)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Global average pooling over time\n",
        "        x = x.mean(dim=1)              # (batch, d_model)\n",
        "\n",
        "        # Classify\n",
        "        x = self.classifier(x)         # (batch, n_classes)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1N_VDMNXgL2d"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# MODEL: CNN BASELINE (for comparison)\n",
        "# =============================================\n",
        "\n",
        "class NIRSCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple 1D CNN baseline for comparison.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels=80, n_classes=5, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv1d(n_channels, 64, kernel_size=7, padding=3),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(128, 128, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.GELU(),\n",
        "            nn.MaxPool1d(2),\n",
        "            nn.Dropout(dropout),\n",
        "\n",
        "            nn.Conv1d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.GELU(),\n",
        "            nn.AdaptiveAvgPool1d(1),\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(64, 64),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(64, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.transpose(1, 2)          # (batch, 80, 72)\n",
        "        x = self.features(x)           # (batch, 64, 1)\n",
        "        x = x.squeeze(-1)              # (batch, 64)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "xHfYzy0Ignbs"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# TRAINING UTILITIES\n",
        "# =============================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (helps with transformer stability)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        total += len(y)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += len(y)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / total, correct / total, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# CROSS-VALIDATION TRAINING\n",
        "# =============================================\n",
        "\n",
        "def train_cv(model_class, model_name, data, labels, subjects, groups,\n",
        "             device, n_folds=N_FOLDS, subject_wise=False):\n",
        "    \"\"\"\n",
        "    Train with K-fold cross-validation.\n",
        "    Returns per-fold accuracies.\n",
        "    \"\"\"\n",
        "    if subject_wise:\n",
        "        n_actual_folds = min(n_folds, len(np.unique(groups)))\n",
        "        kf = GroupKFold(n_splits=n_actual_folds)\n",
        "        splits = list(kf.split(data, labels, groups))\n",
        "        cv_name = f\"Subject-wise {n_actual_folds}-fold\"\n",
        "    else:\n",
        "        kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
        "        splits = list(kf.split(data, labels))\n",
        "        cv_name = f\"Stratified {n_folds}-fold\"\n",
        "\n",
        "    P(f\"\\n  {model_name} — {cv_name} CV\")\n",
        "    P(f\"  {'─' * 50}\")\n",
        "\n",
        "    fold_accs = []\n",
        "    all_test_preds = []\n",
        "    all_test_labels = []\n",
        "    all_histories = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
        "        # Split train into train + val (85/15)\n",
        "        n_train = len(train_idx)\n",
        "        n_val = max(1, int(n_train * 0.15))\n",
        "        np.random.seed(SEED + fold_idx)\n",
        "        perm = np.random.permutation(n_train)\n",
        "        val_subset = train_idx[perm[:n_val]]\n",
        "        train_subset = train_idx[perm[n_val:]]\n",
        "\n",
        "        # Create datasets\n",
        "        train_ds = NIRSDataset(data[train_subset], labels[train_subset],\n",
        "                               subjects[train_subset], augment=True)\n",
        "        val_ds = NIRSDataset(data[val_subset], labels[val_subset],\n",
        "                             subjects[val_subset], augment=False)\n",
        "        test_ds = NIRSDataset(data[test_idx], labels[test_idx],\n",
        "                              subjects[test_idx], augment=False)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                  num_workers=0, pin_memory=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                num_workers=0, pin_memory=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                 num_workers=0, pin_memory=True)\n",
        "\n",
        "        # Create model\n",
        "        model = model_class(n_channels=N_CHANNELS, n_classes=5).to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=EPOCHS, eta_min=1e-6\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Training loop with early stopping\n",
        "        best_val_loss = float('inf')\n",
        "        best_state = None\n",
        "        patience_counter = 0\n",
        "        fold_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, optimizer, criterion, device\n",
        "            )\n",
        "            val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "            scheduler.step()\n",
        "\n",
        "            fold_history['train_loss'].append(train_loss)\n",
        "            fold_history['val_loss'].append(val_loss)\n",
        "            fold_history['train_acc'].append(train_acc)\n",
        "            fold_history['val_acc'].append(val_acc)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= PATIENCE:\n",
        "                break\n",
        "\n",
        "        # Load best model and evaluate on test set\n",
        "        model.load_state_dict(best_state)\n",
        "        model.to(device)\n",
        "        test_loss, test_acc, preds, true_labels = evaluate(\n",
        "            model, test_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        fold_accs.append(test_acc)\n",
        "        all_test_preds.extend(preds)\n",
        "        all_test_labels.extend(true_labels)\n",
        "        all_histories.append(fold_history)\n",
        "\n",
        "        stopped = epoch + 1\n",
        "        P(f\"    Fold {fold_idx+1}: test acc = {test_acc*100:.1f}%  \"\n",
        "          f\"(stopped at epoch {stopped}, best val loss = {best_val_loss:.4f})\")\n",
        "\n",
        "    mean_acc = np.mean(fold_accs)\n",
        "    std_acc = np.std(fold_accs)\n",
        "    P(f\"  ──────────────────────────────────────────────────\")\n",
        "    P(f\"  Mean: {mean_acc*100:.1f}% (+/- {std_acc*100:.1f}%)\")\n",
        "\n",
        "    return mean_acc, std_acc, np.array(all_test_preds), np.array(all_test_labels), all_histories"
      ],
      "metadata": {
        "id": "qPhOwmH0gsXN"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "data_dir = '/content/datasets/robot_control' # Modified line\n",
        "device = torch.device(torch.accelerator.current_accelerator())\n",
        "\n",
        "P(\"=\" * 65)\n",
        "P(\"NIRS Transformer Classifier\")\n",
        "P(\"=\" * 65)\n",
        "P(f\"Data directory: {data_dir}\")\n",
        "\n",
        "data_files = find_data_files(data_dir)\n",
        "if not data_files:\n",
        "    P(f\"ERROR: No .npz files found in {data_dir}\")\n",
        "    sys.exit(1)\n",
        "P(f\"Found {len(data_files)} files\")\n",
        "\n",
        "# =============================================\n",
        "# LOAD & PREPROCESS\n",
        "# =============================================\n",
        "\n",
        "P(\"\\n--- Loading and preprocessing NIRS data ---\")\n",
        "t0 = time.time()\n",
        "\n",
        "all_data = []       # (N, 72, 80)\n",
        "all_labels = []\n",
        "all_subjects = []\n",
        "\n",
        "for i, f in enumerate(data_files):\n",
        "    arr = np.load(f, allow_pickle=True)\n",
        "    label_info = arr['label'].item()\n",
        "    all_labels.append(label_info['label'])\n",
        "    all_subjects.append(label_info.get('subject_id', 'unknown'))\n",
        "\n",
        "    nirs_processed = preprocess_nirs(arr['feature_moments'])  # (72, 80)\n",
        "    all_data.append(nirs_processed)\n",
        "\n",
        "    if (i + 1) % 200 == 0 or (i + 1) == len(data_files):\n",
        "        P(f\"  [{i+1}/{len(data_files)}] {time.time()-t0:.0f}s\")\n",
        "\n",
        "data = np.array(all_data, dtype=np.float32)    # (1395, 72, 80)\n",
        "subjects = np.array(all_subjects)\n",
        "\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(all_labels)\n",
        "\n",
        "# Per-channel z-normalization across the full dataset\n",
        "# (each of the 80 channels gets zero mean, unit variance)\n",
        "for ch in range(data.shape[2]):\n",
        "    ch_data = data[:, :, ch]\n",
        "    mu = ch_data.mean()\n",
        "    sigma = ch_data.std()\n",
        "    if sigma > 1e-10:\n",
        "        data[:, :, ch] = (ch_data - mu) / sigma\n",
        "\n",
        "# Group encoding for subject-wise CV\n",
        "group_enc = LabelEncoder()\n",
        "groups = group_enc.fit_transform(all_subjects)\n",
        "\n",
        "P(f\"\\nDataset:\")\n",
        "P(f\"  Shape:    {data.shape}  (samples, timesteps, channels)\")\n",
        "P(f\"  Classes:  {list(le.classes_)}\")\n",
        "P(f\"  Samples:  {len(labels)}\")\n",
        "P(f\"  Subjects: {len(set(all_subjects))}\")\n",
        "P(f\"  Labels:   {dict(Counter(all_labels))}\")\n",
        "P(f\"  Time:     {time.time()-t0:.0f}s\")\n",
        "\n",
        "# =============================================\n",
        "# MODEL SUMMARIES\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n--- Model architectures ---\")\n",
        "\n",
        "dummy_transformer = NIRSTransformer(n_channels=N_CHANNELS, n_classes=5)\n",
        "dummy_cnn = NIRSCNN(n_channels=N_CHANNELS, n_classes=5)\n",
        "\n",
        "P(f\"\\n  Transformer: {count_parameters(dummy_transformer):,} parameters\")\n",
        "P(f\"    d_model={D_MODEL}, heads={N_HEADS}, layers={N_LAYERS}, ff={FF_DIM}\")\n",
        "P(f\"    dropout={DROPOUT}, patch_embed=Conv1d(5)+Conv1d(3)\")\n",
        "\n",
        "P(f\"\\n  CNN Baseline: {count_parameters(dummy_cnn):,} parameters\")\n",
        "P(f\"    Conv1d(7)→Conv1d(5)→Conv1d(3)→Conv1d(3)→AvgPool→FC\")\n",
        "\n",
        "del dummy_transformer, dummy_cnn\n",
        "\n",
        "# =============================================\n",
        "# TRAINING: STRATIFIED 5-FOLD CV\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"STRATIFIED 5-FOLD CROSS-VALIDATION\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# CNN baseline\n",
        "cnn_acc, cnn_std, cnn_preds, cnn_labels, cnn_histories = train_cv(\n",
        "    NIRSCNN, \"CNN Baseline\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=False\n",
        ")\n",
        "results['CNN (stratified)'] = (cnn_acc, cnn_std)\n",
        "\n",
        "# Transformer\n",
        "tf_acc, tf_std, tf_preds, tf_labels, tf_histories = train_cv(\n",
        "    NIRSTransformer, \"Transformer\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=False\n",
        ")\n",
        "results['Transformer (stratified)'] = (tf_acc, tf_std)\n",
        "\n",
        "# =============================================\n",
        "# TRAINING: SUBJECT-WISE CV\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"SUBJECT-WISE CROSS-VALIDATION (GroupKFold)\")\n",
        "P(\"  Tests generalization to completely new subjects\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "cnn_subj_acc, cnn_subj_std, _, _, cnn_subj_histories = train_cv(\n",
        "    NIRSCNN, \"CNN Baseline\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=True\n",
        ")\n",
        "results['CNN (subject-wise)'] = (cnn_subj_acc, cnn_subj_std)\n",
        "\n",
        "tf_subj_acc, tf_subj_std, tf_subj_preds, tf_subj_labels, tf_subj_histories = train_cv(\n",
        "    NIRSTransformer, \"Transformer\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=True\n",
        ")\n",
        "results['Transformer (subject-wise)'] = (tf_subj_acc, tf_subj_std)\n",
        "\n",
        "# =============================================\n",
        "# DETAILED REPORT (best transformer fold)\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"CLASSIFICATION REPORT (Transformer, stratified CV, all folds combined)\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "P(classification_report(\n",
        "    tf_labels, tf_preds, target_names=le.classes_, digits=3\n",
        "))\n",
        "\n",
        "P(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(tf_labels, tf_preds)\n",
        "header = \"          \" + \"  \".join([f\"{c[:8]:>8s}\" for c in le.classes_])\n",
        "P(header)\n",
        "for i, row in enumerate(cm):\n",
        "    row_str = \"  \".join([f\"{v:8d}\" for v in row])\n",
        "    P(f\"          {row_str}  ← {le.classes_[i]}\")\n",
        "\n",
        "# =============================================\n",
        "# SUMMARY\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"SUMMARY — COMPARISON\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "P(f\"\\n  Previous best (hand-crafted features):\")\n",
        "P(f\"    Stratified 5-fold: 50.3% (SVM)\")\n",
        "P(f\"    Subject-wise:      40.2% (RF)\")\n",
        "\n",
        "P(f\"\\n  Deep learning results:\")\n",
        "for name, (acc, std) in results.items():\n",
        "    marker = \" ★\" if acc > 0.503 else \"\"\n",
        "    P(f\"    {name:35s}: {acc*100:.1f}% (+/- {std*100:.1f}%){marker}\")\n",
        "\n",
        "# =============================================\n",
        "# SAVE BEST MODEL (full training on all data)\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"TRAINING FINAL MODEL ON ALL DATA\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "# Train on all data for deployment\n",
        "full_ds = NIRSDataset(data, labels, subjects, augment=True)\n",
        "full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                            num_workers=0, pin_memory=True)\n",
        "\n",
        "final_model = NIRSTransformer(n_channels=N_CHANNELS, n_classes=5).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    final_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=80, eta_min=1e-6\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "P(f\"  Training for {EPOCHS} epochs on all {len(labels)} samples...\")\n",
        "final_history = {'train_loss': [], 'train_acc': []}\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        final_model, full_loader, optimizer, criterion, device\n",
        "    )\n",
        "    scheduler.step()\n",
        "    final_history['train_loss'].append(train_loss)\n",
        "    final_history['train_acc'].append(train_acc)\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        P(f\"    Epoch {epoch+1:3d}: loss={train_loss:.4f}  acc={train_acc*100:.1f}%\")\n",
        "\n",
        "# Save\n",
        "save_dir = os.getcwd()\n",
        "save_path = os.path.join(save_dir, 'nirs_transformer.pt')\n",
        "torch.save({\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'label_encoder_classes': le.classes_.tolist(),\n",
        "    'n_channels': N_CHANNELS,\n",
        "    'n_classes': 5,\n",
        "    'd_model': D_MODEL,\n",
        "    'n_heads': N_HEADS,\n",
        "    'n_layers': N_LAYERS,\n",
        "    'ff_dim': FF_DIM,\n",
        "}, save_path)\n",
        "P(f\"\\n  Model saved to {save_path}\")\n",
        "\n",
        "# =============================================\n",
        "# PLOT LOSS CURVES\n",
        "# =============================================\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "# Helper to plot fold histories\n",
        "def plot_histories(ax, histories, metric, title, ylabel):\n",
        "    for i, h in enumerate(histories):\n",
        "        ax.plot(h[metric], alpha=0.7, label=f'Fold {i+1}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.legend(fontsize=7)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Row 1: Transformer stratified CV\n",
        "plot_histories(axes[0, 0], tf_histories, 'train_loss',\n",
        "                'Transformer (Stratified CV) — Training Loss', 'Loss')\n",
        "plot_histories(axes[0, 1], tf_histories, 'val_loss',\n",
        "                'Transformer (Stratified CV) — Validation Loss', 'Loss')\n",
        "\n",
        "# Row 2: CNN stratified CV\n",
        "plot_histories(axes[1, 0], cnn_histories, 'train_loss',\n",
        "                'CNN Baseline (Stratified CV) — Training Loss', 'Loss')\n",
        "plot_histories(axes[1, 1], cnn_histories, 'val_loss',\n",
        "                'CNN Baseline (Stratified CV) — Validation Loss', 'Loss')\n",
        "\n",
        "# Row 3: Final model training + accuracy comparison\n",
        "ax = axes[2, 0]\n",
        "ax.plot(final_history['train_loss'], color='steelblue', label='Train Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Final Model Training (all data)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(final_history['train_acc'], color='darkorange', label='Train Acc')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend(loc='center right')\n",
        "\n",
        "ax = axes[2, 1]\n",
        "model_names = list(results.keys())\n",
        "accs = [results[n][0] * 100 for n in model_names]\n",
        "stds = [results[n][1] * 100 for n in model_names]\n",
        "bars = ax.bar(range(len(model_names)), accs, yerr=stds, capsize=4,\n",
        "                color=['steelblue', 'darkorange', 'steelblue', 'darkorange'])\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Model Comparison')\n",
        "ax.set_xticks(range(len(model_names)))\n",
        "ax.set_xticklabels([n.replace(' (', '\\n(') for n in model_names], fontsize=7)\n",
        "ax.set_ylim(0, 100)\n",
        "for bar, acc in zip(bars, accs):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
        "            f'{acc:.1f}%', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(save_dir, 'loss_curves_transformer.png')\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "P(f\"\\n  Loss curves saved to {plot_path}\")\n",
        "\n",
        "total_time = time.time() - t0\n",
        "P(f\"\\n  Total time: {total_time/60:.1f} minutes\")\n",
        "P(\"=\" * 65)\n",
        "P(\"DONE!\")\n",
        "P(\"=\" * 65)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSAljZIdUxQt",
        "outputId": "a3fed111-c1a7-4b4d-8192-b1fe47e20180"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=================================================================\n",
            "NIRS Transformer Classifier\n",
            "=================================================================\n",
            "Data directory: /content/datasets/robot_control\n",
            "Found 1323 files\n",
            "\n",
            "--- Loading and preprocessing NIRS data ---\n",
            "  [200/1323] 6s\n",
            "  [400/1323] 12s\n",
            "  [600/1323] 17s\n",
            "  [800/1323] 23s\n",
            "  [1000/1323] 28s\n",
            "  [1200/1323] 34s\n",
            "  [1323/1323] 37s\n",
            "\n",
            "Dataset:\n",
            "  Shape:    (1323, 72, 80)  (samples, timesteps, channels)\n",
            "  Classes:  [np.str_('Both Fists'), np.str_('Left Fist'), np.str_('Relax'), np.str_('Right Fist'), np.str_('Tongue Tapping')]\n",
            "  Samples:  1323\n",
            "  Subjects: 16\n",
            "  Labels:   {'Left Fist': 260, 'Relax': 260, 'Both Fists': 267, 'Tongue Tapping': 270, 'Right Fist': 266}\n",
            "  Time:     37s\n",
            "\n",
            "--- Model architectures ---\n",
            "\n",
            "  Transformer: 142,613 parameters\n",
            "    d_model=64, heads=1, layers=5, ff=16\n",
            "    dropout=0.3, patch_embed=Conv1d(5)+Conv1d(3)\n",
            "\n",
            "  CNN Baseline: 156,165 parameters\n",
            "    Conv1d(7)→Conv1d(5)→Conv1d(3)→Conv1d(3)→AvgPool→FC\n",
            "\n",
            "=================================================================\n",
            "STRATIFIED 5-FOLD CROSS-VALIDATION\n",
            "=================================================================\n",
            "\n",
            "  CNN Baseline — Stratified 5-fold CV\n",
            "  ──────────────────────────────────────────────────\n",
            "    Fold 1: test acc = 20.8%  (stopped at epoch 3, best val loss = 1.6168)\n",
            "    Fold 2: test acc = 25.3%  (stopped at epoch 3, best val loss = 1.6005)\n",
            "    Fold 3: test acc = 21.1%  (stopped at epoch 3, best val loss = 1.5952)\n",
            "    Fold 4: test acc = 22.3%  (stopped at epoch 3, best val loss = 1.6140)\n",
            "    Fold 5: test acc = 21.6%  (stopped at epoch 3, best val loss = 1.6056)\n",
            "  ──────────────────────────────────────────────────\n",
            "  Mean: 22.2% (+/- 1.6%)\n",
            "\n",
            "  Transformer — Stratified 5-fold CV\n",
            "  ──────────────────────────────────────────────────\n",
            "    Fold 1: test acc = 30.2%  (stopped at epoch 3, best val loss = 1.5833)\n",
            "    Fold 2: test acc = 24.5%  (stopped at epoch 3, best val loss = 1.5707)\n",
            "    Fold 3: test acc = 29.8%  (stopped at epoch 3, best val loss = 1.5842)\n",
            "    Fold 4: test acc = 23.9%  (stopped at epoch 3, best val loss = 1.6011)\n",
            "    Fold 5: test acc = 26.5%  (stopped at epoch 3, best val loss = 1.5582)\n",
            "  ──────────────────────────────────────────────────\n",
            "  Mean: 27.0% (+/- 2.6%)\n",
            "\n",
            "=================================================================\n",
            "SUBJECT-WISE CROSS-VALIDATION (GroupKFold)\n",
            "  Tests generalization to completely new subjects\n",
            "=================================================================\n",
            "\n",
            "  CNN Baseline — Subject-wise 5-fold CV\n",
            "  ──────────────────────────────────────────────────\n",
            "    Fold 1: test acc = 22.2%  (stopped at epoch 3, best val loss = 1.6010)\n",
            "    Fold 2: test acc = 23.0%  (stopped at epoch 3, best val loss = 1.5981)\n",
            "    Fold 3: test acc = 21.5%  (stopped at epoch 3, best val loss = 1.6074)\n",
            "    Fold 4: test acc = 18.4%  (stopped at epoch 3, best val loss = 1.6022)\n",
            "    Fold 5: test acc = 20.7%  (stopped at epoch 3, best val loss = 1.6048)\n",
            "  ──────────────────────────────────────────────────\n",
            "  Mean: 21.2% (+/- 1.6%)\n",
            "\n",
            "  Transformer — Subject-wise 5-fold CV\n",
            "  ──────────────────────────────────────────────────\n",
            "    Fold 1: test acc = 29.3%  (stopped at epoch 3, best val loss = 1.5582)\n",
            "    Fold 2: test acc = 27.4%  (stopped at epoch 3, best val loss = 1.5896)\n",
            "    Fold 3: test acc = 28.5%  (stopped at epoch 3, best val loss = 1.5722)\n",
            "    Fold 4: test acc = 24.3%  (stopped at epoch 3, best val loss = 1.5786)\n",
            "    Fold 5: test acc = 24.8%  (stopped at epoch 3, best val loss = 1.5772)\n",
            "  ──────────────────────────────────────────────────\n",
            "  Mean: 26.9% (+/- 2.0%)\n",
            "\n",
            "=================================================================\n",
            "CLASSIFICATION REPORT (Transformer, stratified CV, all folds combined)\n",
            "=================================================================\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "    Both Fists      0.251     0.318     0.281       267\n",
            "     Left Fist      0.254     0.265     0.259       260\n",
            "         Relax      0.297     0.169     0.216       260\n",
            "    Right Fist      0.219     0.192     0.204       266\n",
            "Tongue Tapping      0.326     0.400     0.359       270\n",
            "\n",
            "      accuracy                          0.270      1323\n",
            "     macro avg      0.269     0.269     0.264      1323\n",
            "  weighted avg      0.269     0.270     0.264      1323\n",
            "\n",
            "Confusion Matrix:\n",
            "          Both Fis  Left Fis     Relax  Right Fi  Tongue T\n",
            "                85        61        19        50        52  ← Both Fists\n",
            "                77        69        24        38        52  ← Left Fist\n",
            "                44        64        44        55        53  ← Relax\n",
            "                80        35        34        51        66  ← Right Fist\n",
            "                53        43        27        39       108  ← Tongue Tapping\n",
            "\n",
            "=================================================================\n",
            "SUMMARY — COMPARISON\n",
            "=================================================================\n",
            "\n",
            "  Previous best (hand-crafted features):\n",
            "    Stratified 5-fold: 50.3% (SVM)\n",
            "    Subject-wise:      40.2% (RF)\n",
            "\n",
            "  Deep learning results:\n",
            "    CNN (stratified)                   : 22.2% (+/- 1.6%)\n",
            "    Transformer (stratified)           : 27.0% (+/- 2.6%)\n",
            "    CNN (subject-wise)                 : 21.2% (+/- 1.6%)\n",
            "    Transformer (subject-wise)         : 26.9% (+/- 2.0%)\n",
            "\n",
            "=================================================================\n",
            "TRAINING FINAL MODEL ON ALL DATA\n",
            "=================================================================\n",
            "  Training for 3 epochs on all 1323 samples...\n",
            "\n",
            "  Model saved to /content/nirs_transformer.pt\n",
            "\n",
            "  Loss curves saved to /content/loss_curves_transformer.png\n",
            "\n",
            "  Total time: 0.8 minutes\n",
            "=================================================================\n",
            "DONE!\n",
            "=================================================================\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuY_xHHnOYPa",
        "outputId": "16f61171-a70f-4ad4-cf51-ba0ea1dc34a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "curl is already the newest version (7.81.0-1ubuntu1.21).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!sudo apt install git git-lfs curl -yqq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dox12qdtPBJA",
        "outputId": "0c6e856e-7a4f-4b9f-f18c-8344fc8d3be5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'treehacks' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!cd ..\n",
        "!git clone https://github.com/Kaweees/treehacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S7_RgC7QHSt",
        "outputId": "61e41bcf-e0a7-43a1-d517-ab73e258c721"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloading uv 0.10.2 x86_64-unknown-linux-gnu\n",
            "no checksums to verify\n",
            "installing to /usr/local/bin\n",
            "  uv\n",
            "  uvx\n",
            "everything's installed!\n"
          ]
        }
      ],
      "source": [
        "!curl -LsSf https://astral.sh/uv/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH8IcTMHPBLr",
        "outputId": "b6d75e0b-e192-4777-a226-de34681897ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m3 packages\u001b[0m \u001b[2min 46ms\u001b[0m\u001b[0m\n",
            "\u001b[2mUsing Python 3.12.12 environment at: /usr\u001b[0m\n",
            "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 45ms\u001b[0m\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!uv pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu130\n",
        "\n",
        "!uv pip install sympy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EouRpOkvPBOA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "import math\n",
        "from collections import Counter\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use('Agg')\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def P(msg=\"\"):\n",
        "    print(msg, flush=True)\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# CONFIG\n",
        "# =============================================\n",
        "\n",
        "N_CHANNELS = 80  # will be updated after loading data\n",
        "\n",
        "# Training config\n",
        "BATCH_SIZE = 256 # depends on machine arch\n",
        "EPOCHS = 3\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 0.01\n",
        "PATIENCE = 20                   # early stopping\n",
        "N_FOLDS = 5\n",
        "SEED = 42\n",
        "\n",
        "# Model config\n",
        "D_MODEL = 64                    # transformer hidden dimension\n",
        "N_HEADS = 1                     # attention heads\n",
        "N_LAYERS = 5                    # transformer layers\n",
        "FF_DIM = 16                    # feedforward dimension\n",
        "DROPOUT = 0.3                   # regularization (aggressive for small dataset)\n",
        "\n",
        "# Augmentation\n",
        "AUG_NOISE_STD = 0.02            # Gaussian noise\n",
        "AUG_TIME_SHIFT = 3              # max time shift (samples)\n",
        "AUG_CHANNEL_DROP = 0.1          # probability of zeroing a channel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3Kh_wElCVJ8R"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# DATASET\n",
        "# =============================================\n",
        "\n",
        "class NIRSDataset(Dataset):\n",
        "    \"\"\"PyTorch dataset for NIRS time-series data.\"\"\"\n",
        "\n",
        "    def __init__(self, data, labels, subjects, augment=False):\n",
        "        \"\"\"\n",
        "        data:     np.array (N, 72, 80) — preprocessed NIRS\n",
        "        labels:   np.array (N,) — integer labels\n",
        "        subjects: np.array (N,) — subject IDs (for subject-wise CV)\n",
        "        augment:  bool — whether to apply data augmentation\n",
        "        \"\"\"\n",
        "        self.data = torch.FloatTensor(data)\n",
        "        self.labels = torch.LongTensor(labels)\n",
        "        self.subjects = subjects\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx].clone()  # (72, 80)\n",
        "        y = self.labels[idx]\n",
        "\n",
        "        if self.augment:\n",
        "            # 1. Gaussian noise\n",
        "            x = x + torch.randn_like(x) * AUG_NOISE_STD\n",
        "\n",
        "            # 2. Random time shift (circular)\n",
        "            shift = torch.randint(-AUG_TIME_SHIFT, AUG_TIME_SHIFT + 1, (1,)).item()\n",
        "            if shift != 0:\n",
        "                x = torch.roll(x, shifts=shift, dims=0)\n",
        "\n",
        "            # 3. Random channel dropout\n",
        "            mask = torch.rand(x.shape[1]) > AUG_CHANNEL_DROP\n",
        "            x = x * mask.unsqueeze(0)\n",
        "\n",
        "        return x, y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1N_VDMNXgL2d"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# MODEL: NIRS TRANSFORMER\n",
        "# =============================================\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"Learnable positional encoding for 72 timesteps.\"\"\"\n",
        "\n",
        "    def __init__(self, d_model, max_len=100):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, d_model) * 0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model)\n",
        "        return x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "\n",
        "class NIRSTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Small transformer for NIRS time-series classification.\n",
        "\n",
        "    Architecture:\n",
        "      Input (72, 80) → Conv1d patch embed → (72, d_model)\n",
        "                     → Positional encoding\n",
        "                     → Transformer Encoder (N layers)\n",
        "                     → Global Average Pooling → (d_model,)\n",
        "                     → Classification head → (5,)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels=80, n_classes=5, seq_len=72,\n",
        "                 d_model=D_MODEL, n_heads=N_HEADS, n_layers=N_LAYERS,\n",
        "                 ff_dim=FF_DIM, dropout=DROPOUT):\n",
        "        super().__init__()\n",
        "\n",
        "        # Patch embedding: Conv1d to project 80 input channels → d_model\n",
        "        # Kernel size 5 captures ~1 second of NIRS data at 4.76 Hz\n",
        "        self.patch_embed = nn.Sequential(\n",
        "            nn.Conv1d(n_channels, d_model, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Conv1d(d_model, d_model, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm1d(d_model),\n",
        "            nn.GELU(),\n",
        "        )\n",
        "\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len=seq_len)\n",
        "        self.input_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=ff_dim,\n",
        "            dropout=dropout,\n",
        "            activation='gelu',\n",
        "            batch_first=True,\n",
        "            norm_first=True,  # Pre-norm (more stable for small datasets)\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            encoder_layer, num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model, n_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (batch, seq_len=72, n_channels=80)\n",
        "        \"\"\"\n",
        "        # Conv1d expects (batch, channels, seq_len)\n",
        "        x = x.transpose(1, 2)          # (batch, 80, 72)\n",
        "        x = self.patch_embed(x)         # (batch, d_model, 72)\n",
        "        x = x.transpose(1, 2)          # (batch, 72, d_model)\n",
        "\n",
        "        # Positional encoding\n",
        "        x = self.pos_encoding(x)\n",
        "        x = self.input_dropout(x)\n",
        "\n",
        "        # Transformer\n",
        "        x = self.transformer(x)        # (batch, 72, d_model)\n",
        "        x = self.norm(x)\n",
        "\n",
        "        # Global average pooling over time\n",
        "        x = x.mean(dim=1)              # (batch, d_model)\n",
        "\n",
        "        # Classify\n",
        "        x = self.classifier(x)         # (batch, n_classes)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPhOwmH0gsXN"
      },
      "outputs": [],
      "source": [
        "# =============================================\n",
        "# TRAINING UTILITIES\n",
        "# =============================================\n",
        "\n",
        "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping (helps with transformer stability)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "        correct += (logits.argmax(dim=1) == y).sum().item()\n",
        "        total += len(y)\n",
        "\n",
        "    return total_loss / total, correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        logits = model(x)\n",
        "        loss = criterion(logits, y)\n",
        "\n",
        "        total_loss += loss.item() * len(y)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += len(y)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "\n",
        "    return total_loss / total, correct / total, np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "# =============================================\n",
        "# CROSS-VALIDATION TRAINING\n",
        "# =============================================\n",
        "\n",
        "def train_cv(model_class, model_name, data, labels, subjects, groups,\n",
        "             device, n_folds=N_FOLDS, subject_wise=False):\n",
        "    \"\"\"\n",
        "    Train with K-fold cross-validation.\n",
        "    Returns per-fold accuracies.\n",
        "    \"\"\"\n",
        "    if subject_wise:\n",
        "        n_actual_folds = min(n_folds, len(np.unique(groups)))\n",
        "        kf = GroupKFold(n_splits=n_actual_folds)\n",
        "        splits = list(kf.split(data, labels, groups))\n",
        "        cv_name = f\"Subject-wise {n_actual_folds}-fold\"\n",
        "    else:\n",
        "        kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=SEED)\n",
        "        splits = list(kf.split(data, labels))\n",
        "        cv_name = f\"Stratified {n_folds}-fold\"\n",
        "\n",
        "    P(f\"\\n  {model_name} — {cv_name} CV\")\n",
        "    P(f\"  {'─' * 50}\")\n",
        "\n",
        "    fold_accs = []\n",
        "    all_test_preds = []\n",
        "    all_test_labels = []\n",
        "    all_histories = []\n",
        "\n",
        "    for fold_idx, (train_idx, test_idx) in enumerate(splits):\n",
        "        # Split train into train + val (85/15)\n",
        "        n_train = len(train_idx)\n",
        "        n_val = max(1, int(n_train * 0.15))\n",
        "        np.random.seed(SEED + fold_idx)\n",
        "        perm = np.random.permutation(n_train)\n",
        "        val_subset = train_idx[perm[:n_val]]\n",
        "        train_subset = train_idx[perm[n_val:]]\n",
        "\n",
        "        # Create datasets\n",
        "        train_ds = NIRSDataset(data[train_subset], labels[train_subset],\n",
        "                               subjects[train_subset], augment=True)\n",
        "        val_ds = NIRSDataset(data[val_subset], labels[val_subset],\n",
        "                             subjects[val_subset], augment=False)\n",
        "        test_ds = NIRSDataset(data[test_idx], labels[test_idx],\n",
        "                              subjects[test_idx], augment=False)\n",
        "\n",
        "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                                  num_workers=0, pin_memory=True)\n",
        "        val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                num_workers=0, pin_memory=True)\n",
        "        test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "                                 num_workers=0, pin_memory=True)\n",
        "\n",
        "        # Create model\n",
        "        model = model_class(n_channels=N_CHANNELS, n_classes=N_CLASSES).to(device)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            optimizer, T_max=EPOCHS, eta_min=1e-6\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        # Training loop with early stopping\n",
        "        best_val_loss = float('inf')\n",
        "        best_state = None\n",
        "        patience_counter = 0\n",
        "        fold_history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
        "\n",
        "        for epoch in range(EPOCHS):\n",
        "            train_loss, train_acc = train_one_epoch(\n",
        "                model, train_loader, optimizer, criterion, device\n",
        "            )\n",
        "            val_loss, val_acc, _, _ = evaluate(model, val_loader, criterion, device)\n",
        "            scheduler.step()\n",
        "\n",
        "            fold_history['train_loss'].append(train_loss)\n",
        "            fold_history['val_loss'].append(val_loss)\n",
        "            fold_history['train_acc'].append(train_acc)\n",
        "            fold_history['val_acc'].append(val_acc)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= PATIENCE:\n",
        "                break\n",
        "\n",
        "        # Load best model and evaluate on test set\n",
        "        model.load_state_dict(best_state)\n",
        "        model.to(device)\n",
        "        test_loss, test_acc, preds, true_labels = evaluate(\n",
        "            model, test_loader, criterion, device\n",
        "        )\n",
        "\n",
        "        fold_accs.append(test_acc)\n",
        "        all_test_preds.extend(preds)\n",
        "        all_test_labels.extend(true_labels)\n",
        "        all_histories.append(fold_history)\n",
        "\n",
        "        stopped = epoch + 1\n",
        "        P(f\"    Fold {fold_idx+1}: test acc = {test_acc*100:.1f}%  \"\n",
        "          f\"(stopped at epoch {stopped}, best val loss = {best_val_loss:.4f})\")\n",
        "\n",
        "    mean_acc = np.mean(fold_accs)\n",
        "    std_acc = np.std(fold_accs)\n",
        "    P(f\"  ──────────────────────────────────────────────────\")\n",
        "    P(f\"  Mean: {mean_acc*100:.1f}% (+/- {std_acc*100:.1f}%)\")\n",
        "\n",
        "    return mean_acc, std_acc, np.array(all_test_preds), np.array(all_test_labels), all_histories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSAljZIdUxQt",
        "outputId": "ced73aa8-7aed-498c-da54-93024c5a7281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================\n",
            "NIRS Transformer Classifier\n",
            "=================================================================\n",
            "Data path: treehacks/preprocessing/dataset_3class_norm.npz\n",
            "\n",
            "--- Loading preprocessed NIRS data ---\n",
            "\n",
            "Dataset:\n",
            "  Shape:    (226800, 1, 726)  (samples, seq_len, features/channels)\n",
            "  Classes:  3 — [np.str_('class_0'), np.str_('class_1'), np.str_('class_2')]\n",
            "  Samples:  226800\n",
            "  Subjects: 14\n",
            "  Labels:   {0: 75600, 2: 75600, 1: 75600}\n",
            "  Time:     1s\n",
            "\n",
            "--- Model architectures ---\n",
            "\n",
            "  Transformer: 344,659 parameters\n",
            "    d_model=64, heads=1, layers=5, ff=16\n",
            "    dropout=0.3, patch_embed=Conv1d(5)+Conv1d(3)\n",
            "    Conv1d(7)→Conv1d(5)→Conv1d(3)→Conv1d(3)→AvgPool→FC\n",
            "\n",
            "=================================================================\n",
            "STRATIFIED 5-FOLD CROSS-VALIDATION\n",
            "=================================================================\n",
            "\n",
            "  Transformer — Stratified 5-fold CV\n",
            "  ──────────────────────────────────────────────────\n",
            "    Fold 1: test acc = 35.9%  (stopped at epoch 3, best val loss = 1.0865)\n",
            "    Fold 2: test acc = 36.2%  (stopped at epoch 3, best val loss = 1.0857)\n",
            "    Fold 3: test acc = 36.1%  (stopped at epoch 3, best val loss = 1.0839)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "data_path = 'treehacks/preprocessing/dataset_3class_norm.npz'\n",
        "device = torch.device(torch.accelerator.current_accelerator())\n",
        "\n",
        "P(\"=\" * 65)\n",
        "P(\"NIRS Transformer Classifier\")\n",
        "P(\"=\" * 65)\n",
        "P(f\"Data path: {data_path}\")\n",
        "\n",
        "# =============================================\n",
        "# LOAD PREPROCESSED DATASET\n",
        "# =============================================\n",
        "\n",
        "P(\"\\n--- Loading preprocessed NIRS data ---\")\n",
        "t0 = time.time()\n",
        "\n",
        "dataset = np.load(data_path, allow_pickle=True)\n",
        "raw_features = dataset['features'].astype(np.float32)  # (N, 726) — already normalized\n",
        "labels = dataset['labels'].astype(np.int64)     # (N,) — 0, 1, 2\n",
        "subjects = dataset['subjects']                   # (N,) — subject IDs\n",
        "\n",
        "ADAPTED_N_CHANNELS = raw_features.shape[1] # 726\n",
        "ADAPTED_SEQ_LEN = 1\n",
        "\n",
        "# Reshape data to (N, ADAPTED_SEQ_LEN, ADAPTED_N_CHANNELS)\n",
        "data = raw_features.reshape(len(labels), ADAPTED_SEQ_LEN, ADAPTED_N_CHANNELS)\n",
        "\n",
        "N_CHANNELS = ADAPTED_N_CHANNELS\n",
        "N_CLASSES = len(np.unique(labels))\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(labels)\n",
        "le.classes_ = np.array([f'class_{i}' for i in range(N_CLASSES)])\n",
        "\n",
        "# Group encoding for subject-wise CV\n",
        "group_enc = LabelEncoder()\n",
        "groups = group_enc.fit_transform(subjects)\n",
        "\n",
        "P(f\"\\nDataset:\")\n",
        "P(f\"  Shape:    {data.shape}  (samples, seq_len, features/channels)\")\n",
        "P(f\"  Classes:  {N_CLASSES} — {list(le.classes_)}\")\n",
        "P(f\"  Samples:  {len(labels)}\")\n",
        "P(f\"  Subjects: {len(np.unique(subjects))}\")\n",
        "P(f\"  Labels:   {dict(Counter(labels.tolist()))}\")\n",
        "P(f\"  Time:     {time.time()-t0:.0f}s\")\n",
        "\n",
        "# =============================================\n",
        "# MODEL SUMMARIES\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n--- Model architectures ---\")\n",
        "\n",
        "dummy_transformer = NIRSTransformer(n_channels=N_CHANNELS, n_classes=N_CLASSES, seq_len=ADAPTED_SEQ_LEN)\n",
        "\n",
        "P(f\"\\n  Transformer: {count_parameters(dummy_transformer):,} parameters\")\n",
        "P(f\"    d_model={D_MODEL}, heads={N_HEADS}, layers={N_LAYERS}, ff={FF_DIM}\")\n",
        "P(f\"    dropout={DROPOUT}, patch_embed=Conv1d(5)+Conv1d(3)\")\n",
        "\n",
        "P(f\"    Conv1d(7)→Conv1d(5)→Conv1d(3)→Conv1d(3)→AvgPool→FC\")\n",
        "\n",
        "del dummy_transformer\n",
        "\n",
        "# =============================================\n",
        "# TRAINING: STRATIFIED 5-FOLD CV\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"STRATIFIED 5-FOLD CROSS-VALIDATION\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "results = {}\n",
        "\n",
        "# Transformer\n",
        "tf_acc, tf_std, tf_preds, tf_labels, tf_histories = train_cv(\n",
        "    lambda n_channels, n_classes: NIRSTransformer(n_channels=n_channels, n_classes=n_classes, seq_len=ADAPTED_SEQ_LEN),\n",
        "    \"Transformer\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=False\n",
        ")\n",
        "results['Transformer (stratified)'] = (tf_acc, tf_std)\n",
        "\n",
        "# =============================================\n",
        "# TRAINING: SUBJECT-WISE CV\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"SUBJECT-WISE CROSS-VALIDATION (GroupKFold)\")\n",
        "P(\"  Tests generalization to completely new subjects\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "tf_subj_acc, tf_subj_std, tf_subj_preds, tf_subj_labels, tf_subj_histories = train_cv(\n",
        "    lambda n_channels, n_classes: NIRSTransformer(n_channels=n_channels, n_classes=n_classes, seq_len=ADAPTED_SEQ_LEN),\n",
        "    \"Transformer\", data, labels, subjects, groups, device,\n",
        "    n_folds=N_FOLDS, subject_wise=True\n",
        ")\n",
        "results['Transformer (subject-wise)'] = (tf_subj_acc, tf_subj_std)\n",
        "\n",
        "# =============================================\n",
        "# DETAILED REPORT (best transformer fold)\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"CLASSIFICATION REPORT (Transformer, stratified CV, all folds combined)\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "P(classification_report(\n",
        "    tf_labels, tf_preds, target_names=le.classes_, digits=3\n",
        "))\n",
        "\n",
        "P(\"Confusion Matrix:\")\n",
        "cm = confusion_matrix(tf_labels, tf_preds)\n",
        "header = \"          \" + \"  \".join([f\"{c[:8]:>8s}\" for c in le.classes_])\n",
        "P(header)\n",
        "for i, row in enumerate(cm):\n",
        "    row_str = \"  \".join([f\"{v:8d}\" for v in row])\n",
        "    P(f\"          {row_str}  ← {le.classes_[i]}\")\n",
        "\n",
        "# =============================================\n",
        "# SUMMARY\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"SUMMARY — COMPARISON\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "P(f\"\\n  Deep learning results:\")\n",
        "for name, (acc, std) in results.items():\n",
        "    P(f\"    {name:35s}: {acc*100:.1f}% (+/- {std*100:.1f}%)\")\n",
        "\n",
        "# =============================================\n",
        "# SAVE BEST MODEL (full training on all data)\n",
        "# =============================================\n",
        "\n",
        "P(f\"\\n{'=' * 65}\")\n",
        "P(\"TRAINING FINAL MODEL ON ALL DATA\")\n",
        "P(\"=\" * 65)\n",
        "\n",
        "# Train on all data for deployment\n",
        "full_ds = NIRSDataset(data, labels, subjects, augment=True)\n",
        "full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "                            num_workers=0, pin_memory=True)\n",
        "\n",
        "final_model = NIRSTransformer(n_channels=N_CHANNELS, n_classes=N_CLASSES, seq_len=ADAPTED_SEQ_LEN).to(device)\n",
        "optimizer = torch.optim.AdamW(\n",
        "    final_model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY\n",
        ")\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer, T_max=80, eta_min=1e-6\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "P(f\"  Training for {EPOCHS} epochs on all {len(labels)} samples...\")\n",
        "final_history = {'train_loss': [], 'train_acc': []}\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train_one_epoch(\n",
        "        final_model, full_loader, optimizer, criterion, device\n",
        "    )\n",
        "    scheduler.step()\n",
        "    final_history['train_loss'].append(train_loss)\n",
        "    final_history['train_acc'].append(train_acc)\n",
        "    if (epoch + 1) % 20 == 0:\n",
        "        P(f\"    Epoch {epoch+1:3d}: loss={train_loss:.4f}  acc={train_acc*100:.1f}%\")\n",
        "\n",
        "# Save\n",
        "save_dir = os.getcwd()\n",
        "save_path = os.path.join(save_dir, 'nirs_transformer.pt')\n",
        "torch.save({\n",
        "    'model_state_dict': final_model.state_dict(),\n",
        "    'label_encoder_classes': le.classes_.tolist(),\n",
        "    'n_channels': N_CHANNELS,\n",
        "    'n_classes': N_CLASSES,\n",
        "    'd_model': D_MODEL,\n",
        "    'n_heads': N_HEADS,\n",
        "    'n_layers': N_LAYERS,\n",
        "    'ff_dim': FF_DIM,\n",
        "    'seq_len': ADAPTED_SEQ_LEN,\n",
        "}, save_path)\n",
        "P(f\"\\n  Model saved to {save_path}\")\n",
        "\n",
        "# =============================================\n",
        "# PLOT LOSS CURVES\n",
        "# =============================================\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "\n",
        "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
        "\n",
        "# Helper to plot fold histories\n",
        "def plot_histories(ax, histories, metric, title, ylabel):\n",
        "    for i, h in enumerate(histories):\n",
        "        ax.plot(h[metric], alpha=0.7, label=f'Fold {i+1}')\n",
        "    ax.set_xlabel('Epoch')\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_title(title)\n",
        "    ax.legend(fontsize=7)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Row 1: Transformer stratified CV\n",
        "plot_histories(axes[0, 0], tf_histories, 'train_loss',\n",
        "                'Transformer (Stratified CV) — Training Loss', 'Loss')\n",
        "plot_histories(axes[0, 1], tf_histories, 'val_loss',\n",
        "                'Transformer (Stratified CV) — Validation Loss', 'Loss')\n",
        "\n",
        "# Row 3: Final model training + accuracy comparison\n",
        "ax = axes[2, 0]\n",
        "ax.plot(final_history['train_loss'], color='steelblue', label='Train Loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.set_title('Final Model Training (all data)')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(final_history['train_acc'], color='darkorange', label='Train Acc')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend(loc='center right')\n",
        "\n",
        "ax = axes[2, 1]\n",
        "model_names = list(results.keys())\n",
        "accs = [results[n][0] * 100 for n in model_names]\n",
        "stds = [results[n][1] * 100 for n in model_names]\n",
        "bars = ax.bar(range(len(model_names)), accs, yerr=stds, capsize=4,\n",
        "                color=['steelblue', 'darkorange', 'steelblue', 'darkorange'])\n",
        "ax.set_ylabel('Accuracy (%)')\n",
        "ax.set_title('Model Comparison')\n",
        "ax.set_xticks(range(len(model_names)))\n",
        "ax.set_xticklabels([n.replace(' (', '\\n(') for n in model_names], fontsize=7)\n",
        "ax.set_ylim(0, 100)\n",
        "for bar, acc in zip(bars, accs):\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1,\n",
        "            f'{acc:.1f}%', ha='center', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plot_path = os.path.join(save_dir, 'loss_curves_transformer.png')\n",
        "plt.savefig(plot_path, dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "P(f\"\\n  Loss curves saved to {plot_path}\")\n",
        "\n",
        "total_time = time.time() - t0\n",
        "P(f\"\\n  Total time: {total_time/60:.1f} minutes\")\n",
        "P(\"=\" * 65)\n",
        "P(\"DONE!\")\n",
        "P(\"=\" * 65)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
